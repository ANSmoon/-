{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Author : 문범수\n",
    "##### date : 2024-02-18\n",
    "##### title : 02/18수업 복습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 전처리 (Data Preprocessing)\n",
    "\n",
    "\n",
    "- ML의 모든 알고리즘은 데이터 기반, 따라서 어떤 데이터를 입력으로 가지느냐에 따라 결과도 크게 달라질 수 있다.\n",
    " >Garbage In, Garbage Out\n",
    " \n",
    "- Null 값은 어떻게 처리할 것인가?\n",
    "-  - 단순히 평균값 대체? 해당 피처의 중요도가 높다면 피처의 평균으로 대처할 경우 왜곡 심할 수 있음. 따라서 업무 로직등을 상세히 검토해 더 정밀한 대체값을 선정\n",
    "\n",
    "\n",
    "- 사이킷런의 머신러닝 알고리즘은 문자열 값을 입력값으로 허용하지 않음, 따라서 문자형 값은 인코딩돼서 숫자 형으로 변환해야 함\n",
    "- - 불필요한 피처라고 생각되면 삭제하는 게 더 좋음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 인코딩\n",
    "\n",
    "### 레이블 인코딩(Label encoding)\n",
    "- 카테고리 피처를 코드형 숫자 값으로 변환 ex)상품 데이터의 상품 구분이 TV, 냉장고, 전자레인지, 컴퓨터 등 TV :1 , 냉장고 :2 전자레인지 :3, 컴퓨터 :4\n",
    "### 원-핫 인코딩(One Hot encoding)\n",
    "- 피처 값의 유형에 따라 새로운 피처를 추가해 고유 값에 해당하는 컬럼에만 1을 표시하고 나머지 컬럼에는 0을 표시하는 방식\n",
    "### 빈도인코딩 Frequency Encoding\n",
    "### SVD 잠재 의미 분석하는 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#레이블 인코딩 예시\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "items = ['TV','냉장고','전자레인지','노트북','선풍기','선풍기','믹서기','믹서기']\n",
    "\n",
    "#LabelEncoder를 객체로 생성한 후, fit()과 transform()으로 레이블 인코딩 수행.\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(items)\n",
    "labels = encoder.transform(items)\n",
    "print('인코딩 변환값:',labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터가 많은 경우\n",
    "print('인코딩 클래스:', encoder.classes_)\n",
    "#0부터 순서대로 인코딩 값에 대한 원본을 가지고 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#다시 디코딩 하는 법\n",
    "print('디코딩 원본값:',encoder.inverse_transform([4,5,2,0,1,1,3,3,2,4,4,4,4,4,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('레이블인코딩예시.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 주의할 점\n",
    "- 레이블 인코딩은 간단하게 문자열 값을 숫자형 카테고리 값으로 변환\n",
    "- 하지만 레이블 인코딩이 일괄적인 숫자 값으로 변환이 되면서 몇몇 ML 알고리즘에는 이를 적용할 경우 예측 성능 떨어지는 경우 발생\n",
    "- 1과 2의 차이가 정말 차이가 있는 게 맞는 건가? \n",
    "- 숫자 값에 따른 순서나 중요도로 인식이 될 수 있음, 이런 특성 때문에 레이블 인코딩은 선형회귀와 같은 ML알고리즘에는 적용하지 않아야 함\n",
    "- 트리 계열의 ML알고리즘은 숫자의 이러한 특성을 반영하지 않으므로 레이블 인코딩 별문제 없음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 원-핫 인코딩은 레이블 인코딩의 이러한 문제점을 해결하기 위한 인코딩 방식\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#원-핫 인코딩은 레이블 인코딩의 이러한 문제점을 해결하기 위한 인코딩 방식\n",
    "Image('원-핫인코딩예시.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 해당 고유 값에 메칭되는 피처만 1이 되고 나머지 피처는 0을 입력하며, 이러한 특성으로 원-핫(여러 개의 속성 중 단 한 개의 속성만 1로 표시)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "items = ['TV','냉장고','전자레인지','컴퓨터','선풍기','선풍기','믹서기','믹서기']\n",
    "\n",
    "#2차원 ndarray로 변환\n",
    "items = np.array(items).reshape(-1,1)\n",
    "\n",
    "#원-핫 인코딩을 적용 \n",
    "oh_encoder = OneHotEncoder()\n",
    "oh_encoder.fit(items)\n",
    "oh_labels = oh_encoder.transform(items)\n",
    "\n",
    "#OneHotEncoder로 변환한 결과는 희소행렬이므로 toarray()를 이용해 밀집 행렬로 변환\n",
    "print('원-핫 인코딩 데이터')\n",
    "print(oh_labels.toarray())\n",
    "print('원-핫 인코딩 데이터 차원')\n",
    "print(oh_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 컬럼이 items = ['TV','냉장고','전자레인지','컴퓨터','선풍기','선풍기','믹서기','믹서기'] 로 매칭\n",
    "- 변환된 데이터의 첫 번째 레코드의 첫 번째 레코드의 첫 번째 컬럼이 1이고 나머지는 모두 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.DataFrame({'이름':['홍길동','김영희','김철수','박철수','오철수','정철수','정철수','배철수']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(df_1) # 원핫인코딩 바로 사용 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "df = sns.load_dataset('titanic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sns= df[['sex','embarked','class']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_1= pd.get_dummies(df_sns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1= df_1*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tt=pd.concat([df,df_1],axis=1)#깔끔한 하나의 숫자로 완성된 데이터프레임을 만들 수 있다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#판다스에서 지원하는 API \n",
    "#get_dummies\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'item':['TV','냉장고','전자레인지','컴퓨터','선풍기','선풍기','믹서기','믹서기']})\n",
    "\n",
    "pd.get_dummies(df) #원핫인코딩!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문자열 데이터를 원핫인코더로 바꿔서 컬럼을 추가하는 게 정말로 좋은 것일까?\n",
    "\n",
    "- 우리가 궁극적으로 (원핫)인코딩을 하는 이유는 무엇일까?\n",
    "- 왜 컴퓨터가 이해할 수 있도록 바꾸는 거죠?\n",
    "- 머신러닝 즉 ML 데이터 학습을 하기 위해서 -> 학습한다는데 왜 인코딩을 하죠?\n",
    "- ML 학습을해서 예측을 하기 위한 것\n",
    "- ex)\n",
    "- 타이타닉데이터를 원한 인코딩을해서 결국 생존율 예측하려고 한다.\n",
    "- ML 생존율 예측 정확도나 평가지표를 높이기 위해서 -> 생존율 y값\n",
    "- y 값이라는 것은 해당 인덱스의 최종 target 우리가 예측해야 하는 값 1,0 생존, 사망\n",
    "- 결국 인코딩하는 이유는 컴퓨터에게 y값에 대한 더 정확한 정보를 주어서 예측할 때 그쪽으로 더 잘 예측할 수 있도록 하는 것\n",
    "- 성별 M/F 인코딩 -> 0/1 0/1 \n",
    "- 성별 기타컬럼.. y값\n",
    "- 0 .... .....1\n",
    "- 1 ..........0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 피처 스케일링과 정규화\n",
    "### 피처 스케일링(feature scaling) \n",
    "#### 표준화(Standardization), 정규화(Normalization)\n",
    "\n",
    "- 표준화는 데이터의 피처 각각이 평균이 0 이고 분산이 1인 가우시안 정규 분포를 가진 값으로 변환\n",
    "- 새로운 x의 값은 x의 평균을 뺀 값을 피처 x의 표준편차로 나눈 값으로 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 표준화는 전체 평균을 기준으로 어느 정도 떨어져 있는지 나타낼 때 사용! ( -, 1 이상의 값도 가질 수 있다. )\n",
    "- 정규화는 데이터의 범위를 0부터 1까지로 변환, 데이터 분포를 조정하는 방법 (0,1의 값을 가진다. )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('표준화예시사진.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 일반적으로 정규화는 서로 다른 피처의 크기를 통일하기 위해 크기를 변환해주는 개념\n",
    "- A는 거리를 나타내는 변수 값 0~ 100, B 는 금액 나타나는 속성 0~ 1,000,000,000,000이면 두 변수를 동일한 크기 단위로 비교하기 위해 0~1사이로 변환\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('정규화예시사진.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block\" style=\"border: 2px solid #E65100;background-color:#FFF3E0;padding:10px\">\n",
    "    \n",
    "#사이킷런 전처리에서 제공하는 Normalizer 모듈과 일반적인 정규화는 약간의 차이가 있음\n",
    "- 사이킷런의 Normalizer 모듈은 선형대수에서의 정규화 개념 적용, 개별 벡터의 크기를 맞추기 위해 변환 의미\n",
    "- 즉, 개별 벡터를 모든 피처 벡터의 크기로 나눔, 세 개의 피처 x,y,z가 있으면 새로운 데이터 x_new는 원래 값에서 세 개의 피처의 i번째 피처 값에 해당하는 크기를 합한 값으로 나눠줌 (정규 벡터화)\n",
    "    \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('벡터정규화예시사진.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StandardScaler\n",
    "- 표준화를 쉽게 지원하기 위한 클래스\n",
    "- 즉, 개별 피처를 평균이 0이고, 분산이 1인 값으로 변환 \n",
    "- 이렇게 가우시안 정규 분포를 가질 수 있도록 데이터를 변환하는 것\n",
    "- 사이킷런 RBF 커널 SVM, 선형회귀, 로지스틱 회귀는 데이터가 가우신 반포 가정, 표준화를 적용하는 것은 예측 성능 향상에 중요한 요소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd \n",
    "#붓꽃 데이터 세트를 로딩하고 DataFrame으로 변환\n",
    "iris = load_iris()\n",
    "iris_data = iris.data\n",
    "iris_df = pd.DataFrame(data=iris_data, columns = iris.feature_names)\n",
    "\n",
    "print('feature들의 평균 값')\n",
    "print(iris_df.mean())\n",
    "print('\\nfeature들의 분산 값')\n",
    "print(iris_df.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- StandardScaler 객체를 생성 후에 fit()과 trnasform() 메서드에 변환 대상 피처 데이터 세트를 입력하고 호출하면 간단하게 변환\n",
    "- transform()을 호출할 때 스케일 변환된 데이터 세트가 넘파이의 ndarray이므로 이를 DataFrame으로 변환해 평균값 분산값 다시 확인!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "# StandardScaler객체 생성\n",
    "scaler = StandardScaler()\n",
    "# StandardScaler로 데이터 세트 변환. fit()과 transform() 호출.\n",
    "scaler.fit(iris_df)\n",
    "iris_scaled = scaler.transform(iris_df)\n",
    "\n",
    "#transform()시 스케일 변화노딘 데이터 세트가 Numpy ndarray로 반환돼 이를 DataFrame으로 변환\n",
    "iris_df_scaled = pd.DataFrame(data=iris_scaled, columns=iris.feature_names)\n",
    "print('feature 들의 평균 값')\n",
    "print(iris_df_scaled.mean())\n",
    "print('\\nfeature들의 분산 값')\n",
    "print(iris_df_scaled.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df_standarded= (iris_df - iris_df.mean())/iris_df.std()\n",
    "\n",
    "plt.title('Origin Dataset')\n",
    "plt.hist(iris_df)\n",
    "plt.legend(iris_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Standarded Dataset')\n",
    "plt.hist(iris_df_standarded)\n",
    "plt.legend(iris_df_scaled.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모든 컬럼 값의 평균이 0에 아주 까가운 값으로, 분산은 1에 아주 까가운 값으로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MinMaxScaler\n",
    "- 데이터 값을 0과 1사이의 범위 값으로 변환 (음수 값이면 -1에서 1값으로 변환)\n",
    "- 데이터 분포가 가우시안 분포가 아닌 경우에 Min,Max Scale을 적용\n",
    "- 분류보다 회귀에 유용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#MinMaxScaler객체 생성\n",
    "scaler = MinMaxScaler()\n",
    "#MinMaxScaler로 데이터 세트 변환. fit()과 transform() 호출\n",
    "scaler.fit(iris_df)\n",
    "iris_scaled = scaler.transform(iris_df)\n",
    "\n",
    "#transform()시 스케일 변환된 데이터 세트가 Numpy ndarray로 반환돼 이를 DataFrame으로 변환\n",
    "iris_df_scaled = pd.DataFrame(data=iris_scaled, columns=iris.feature_names)\n",
    "print('feature들의 최솟값')\n",
    "print(iris_df_scaled.min())\n",
    "print('\\nfeature들의 최댓값')\n",
    "print(iris_df_scaled.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([[166, 18],\n",
    "                  [172, 25],\n",
    "                  [158, 30],\n",
    "                  [182, 21],\n",
    "                  [161, 26],\n",
    "                  [155, 15]])\n",
    "df.columns = ['height', 'age']\n",
    "df_new = (df - df.min())/(df.max() - df.min())\n",
    "\n",
    "# 시각화\n",
    "plt.title('Origin Dataset')\n",
    "plt.plot(df)\n",
    "plt.legend(df.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Scaled Dataset')\n",
    "plt.plot(df_new)\n",
    "plt.legend(df_new.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모든 피처에 0에서 1사이의 값으로 변환되는 스케일링 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MaxAbsScaler()\n",
    "- 각 특성의 절대값이 0 과 1 사이가 되도록 스케일링합니다.\n",
    "- 즉, 모든 값은 -1 과 1 사이로 표현되며, 데이터가 양수일 경우 MinMaxScaler 와 같습니다.\n",
    "- 이상치에 매우 민감합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "# 변형 객체 생성\n",
    "maxabs_scaler = MaxAbsScaler()\n",
    "\n",
    "# 훈련데이터의 모수 분포 저장\n",
    "maxabs_scaler.fit(iris_df)\n",
    "\n",
    "# 훈련 데이터 스케일링\n",
    "iris_scaled = maxabs_scaler.transform(iris_df)\n",
    "\n",
    "#transform()시 스케일 변환된 데이터 세트가 Numpy ndarray로 반환돼 이를 DataFrame으로 변환\n",
    "iris_df_scaled = pd.DataFrame(data=iris_scaled, columns=iris.feature_names)\n",
    "print('feature들의 최솟값')\n",
    "print(iris_df_scaled.min())\n",
    "print('\\nfeature들의 최댓값')\n",
    "print(iris_df_scaled.max())\n",
    "\n",
    "# 테스트 데이터의 스케일링\n",
    "#X_test_scaled = maxabs_scaler.transform(X_test)\n",
    "\n",
    "# 스케일링 된 결과 값으로 본래 값을 구할 수도 있다.\n",
    "# X_origin = maxabs_scaler.inverse_transform(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([[1.0, 18],\n",
    "                  [0.8, 25],\n",
    "                  [-0.5, 30],\n",
    "                  [-1.8, 21],\n",
    "                  [1.2, 26],\n",
    "                  [0.6, 15]])\n",
    "df.columns = ['sight', 'age']\n",
    "df_new = df/df.abs().max()\n",
    "\n",
    "# 시각화\n",
    "plt.title('Origin Dataset')\n",
    "plt.plot(df)\n",
    "plt.legend(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.title('Scaled Dataset')\n",
    "plt.plot(df_new)\n",
    "plt.legend(df_new.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RobustScaler()\n",
    "- 평균과 분산 대신에 중간 값과 사분위 값을 사용합니다.\n",
    "- 중간 값은 정렬시 중간에 있는 값을 의미하고\n",
    "- 사분위값은 1/4, 3/4에 위치한 값을 의미합니다.\n",
    "- 이상치 영향을 최소화할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# 변형 객체 생성\n",
    "robust_scaler = RobustScaler()\n",
    "\n",
    "# 훈련데이터의 모수 분포 저장\n",
    "robust_scaler.fit(iris_df)\n",
    "\n",
    "# 훈련 데이터 스케일링\n",
    "iris_scaled = robust_scaler.transform(iris_df)\n",
    "\n",
    "# 테스트 데이터의 스케일링\n",
    "#X_test_scaled = robust_scaler.transform(X_test)\n",
    "\n",
    "# 스케일링 된 결과 값으로 본래 값을 구할 수도 있다.\n",
    "# X_origin = robust_scaler.inverse_transform(X_train_scaled)\n",
    "\n",
    "\n",
    "#transform()시 스케일 변환된 데이터 세트가 Numpy ndarray로 반환돼 이를 DataFrame으로 변환\n",
    "iris_df_scaled = pd.DataFrame(data=iris_scaled, columns=iris.feature_names)\n",
    "print('feature들의 최솟값')\n",
    "print(iris_df_scaled.min())\n",
    "print('\\nfeature들의 최댓값')\n",
    "print(iris_df_scaled.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['pclass','fare']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizer()\n",
    "- 앞의 4가지 스케일러는 각 특성(열)의 통계치를 이용하여 진행됩니다.\n",
    "- 그러나 Normalizer 의 경우 각 샘플(행)마다 적용되는 방식입니다.\n",
    "- 이는 한 행의 모든 특성들 사이의 유클리드 거리(L2 norm)가 1이 되도록 스케일링합니다.\n",
    "- 일반적인 데이터 전처리의 상황에서 사용되는 것이 아니라\n",
    "- 모델(특히나 딥러닝) 내 학습 벡터에 적용하며,\n",
    "- 특히나 피쳐들이 다른 단위(키, 나이, 소득 등)라면 더더욱 사용하지 않습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \n",
    "    - 소득 나이 통장개수 잔액 \n",
    "    - 1000000 50 5  2000\n",
    "    - 50000000 40 3 3000\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# 변형 객체 생성\n",
    "normal_scaler = Normalizer()\n",
    "\n",
    "# 훈련데이터의 모수 분포 저장\n",
    "normal_scaler.fit(iris_df)\n",
    "\n",
    "# 훈련 데이터 스케일링\n",
    "X_train_scaled = normal_scaler.transform(iris_df)\n",
    "#transform()시 스케일 변환된 데이터 세트가 Numpy ndarray로 반환돼 이를 DataFrame으로 변환\n",
    "iris_df_scaled = pd.DataFrame(data=iris_scaled, columns=iris.feature_names)\n",
    "print('feature들의 최솟값')\n",
    "print(iris_df_scaled.min())\n",
    "print('\\nfeature들의 최댓값')\n",
    "print(iris_df_scaled.max())\n",
    "# 테스트 데이터의 스케일링\n",
    "#X_test_scaled = normal_scaler.transform(X_test)\n",
    "\n",
    "# 스케일링 된 결과 값으로 본래 값을 구할 수도 있다.\n",
    "# X_origin = normal_scaler.inverse_transform(X_train_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature_selection \n",
    "- 만든 피처를 선택해야 한다.\n",
    "- 상관계수, y값에 따라 볼 수 있고, 통계적으로도 볼 수 있고 기타 방법 많은데\n",
    "- 0,1 이진분류로 진행했을 때 간단한 threshold를 가지고 피처를 선택할 수 있는 방법을 진행할 예정\n",
    "\n",
    "- sklearn.feature_selection.VarianceThreshold\n",
    "\n",
    "- 내가 원하는 threshold 값을 통해서 피처를 선택한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "X = [[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1]]\n",
    "sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "sel.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tt =df_tt[['survived','sex_female','sex_male','embarked_C','embarked_Q','embarked_S','class_First','class_Second','class_Third']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = VarianceThreshold(threshold=(0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel.fit_transform(df_tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = VarianceThreshold(threshold=(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel.fit_transform(df_tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = VarianceThreshold(threshold=(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tt_ft=df_tt[['survived','sex_female','sex_male','embarked_S','class_Third']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- embarked_C, embarked_Q,class_First, class_Second (제거가 된 나머지 피처만 확인 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "= sel.fit_transform(df_tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel.get_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel.get_feature_names_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel.feature_names_in_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel.n_features_in_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- feature selection 을 통해서 간단한 성능을 비교!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_clf = DecisionTreeClassifier(random_state=111)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tt_x=df_tt[[ 'sex_female', 'sex_male', 'embarked_C', 'embarked_Q',\n",
    "       'embarked_S', 'class_First', 'class_Second', 'class_Third']]X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_tt_x, df_tt['survived'], test_size=0.3, random_state=111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#학습\n",
    "dt_clf = dt_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = dt_clf.predict(X_test)\n",
    "ac_1 = accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ac_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 피처 셀렉션을 통한 해당 피처만 추출하고 정확도를 평가한 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tt_ft_x=df_tt_ft[['sex_female', 'sex_male', 'embarked_S', 'class_Third']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_tt_ft_x, df_tt_ft['survived'], test_size=0.3, random_state=111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#학습\n",
    "dt_clf = dt_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = dt_clf.predict(X_test)\n",
    "ac_1 = accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ac_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
